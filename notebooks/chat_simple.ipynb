{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"openai.gpt-oss-120b-1:0\",\n",
    "    # model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    # model_id=\"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "    model_kwargs=dict(temperature=0.2),\n",
    "    region=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843153f1",
   "metadata": {},
   "source": [
    "# Using Checkpoint for Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27fb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Create an in-memory checkpointer\n",
    "memory = InMemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "thread_id = \"user_session_1\"\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02db96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <reasoning>The user says \"Hello! My name is Dan.\" Likely they want a greeting. We respond friendly. Ask how can help.</reasoning>Hi Dan! üëã Nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "stream_graph_updates(\"Hello! My name is Dan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <reasoning>The user says \"I am a Virgo.\" Probably they want to talk about zodiac sign. We can respond acknowledging and maybe ask about traits or what they'd like to discuss. Keep friendly.</reasoning>Got it‚ÄîVirgo it is! ‚ôçÔ∏è  \n",
      "Virgos are often described as detail‚Äëoriented, practical, and analytical. Do you feel those traits describe you, or is there something specific about being a Virgo you‚Äôd like to chat about?\n"
     ]
    }
   ],
   "source": [
    "stream_graph_updates(\"I am a Virgo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96edbac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <reasoning>User: \"Guess which month I was born?\" They said they are a Virgo. Virgo dates are August 23 ‚Äì September 22. So month could be August or September. Could guess one. Maybe ask if August or September. Provide guess.</reasoning>Since you mentioned you‚Äôre a Virgo (August‚ÄØ23‚ÄØ‚Äì‚ÄØSeptember‚ÄØ22), I‚Äôll guess you were born in **September**. üåü Was I close?\n"
     ]
    }
   ],
   "source": [
    "stream_graph_updates(\"Guess which month I was born?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68dfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <reasoning>User asks \"Did you get my name?\" We previously responded that name is Dan. So answer confirming we have it. Possibly ask if they'd like to be called something else.</reasoning>Yes‚ÄîI‚Äôve got your name as **Dan**. üòä Let me know if there‚Äôs anything else you‚Äôd like to talk about!\n"
     ]
    }
   ],
   "source": [
    "stream_graph_updates(\"Did you get my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d866e",
   "metadata": {},
   "source": [
    "# Using List for Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    history: str  # conversation history as plain text\n",
    "    user_input: str\n",
    "    response: str\n",
    "\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "\n",
    "def generate_response(state: State):\n",
    "    # Build prompt by combining history and user input\n",
    "    prompt = f\"{state['history']}\\nUser: {state['user_input']}\\nAssistant:\"\n",
    "    # Call the non-chat LLM with the prompt string\n",
    "    completion = llm.invoke(prompt)\n",
    "    # Append assistant response to history\n",
    "    new_history = f\"{prompt} {completion}\"\n",
    "    return {\"response\": completion, \"history\": new_history}\n",
    "\n",
    "\n",
    "graph.add_node(\"generate_response\", generate_response)\n",
    "graph.add_edge(START, \"generate_response\")\n",
    "graph.add_edge(\"generate_response\", END)\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b278b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = {\"history\": \"\", \"user_input\": \"\", \"response\": \"\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9083a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input):\n",
    "    state[\"user_input\"] = user_input\n",
    "    result = app.invoke(state)\n",
    "    print(\"Assistant:\", result[\"response\"])\n",
    "    state[\"history\"] = result[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b002381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: content='<reasoning>The user says \"Hello!\" again. We should respond politely. Probably greet back and ask how can help.</reasoning>Hello! üëã How can I help you today?' additional_kwargs={'usage': {'prompt_tokens': 371, 'completion_tokens': 42, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 413}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} response_metadata={'usage': {'prompt_tokens': 371, 'completion_tokens': 42, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 413}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} id='run--ca9006b9-2594-4667-95d1-e1c3b93c72db-0' usage_metadata={'input_tokens': 371, 'output_tokens': 42, 'total_tokens': 413, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "chat(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b098da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: content='<reasoning>The user says \"My name is Dan\". We should respond acknowledging and perhaps ask how we can help. Keep friendly tone.</reasoning>Nice to meet you, Dan! How can I assist you today?' additional_kwargs={'usage': {'prompt_tokens': 676, 'completion_tokens': 48, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 724}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} response_metadata={'usage': {'prompt_tokens': 676, 'completion_tokens': 48, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 724}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} id='run--70e73097-3d86-455b-ad9a-505e92f2b4cd-0' usage_metadata={'input_tokens': 676, 'output_tokens': 48, 'total_tokens': 724, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "chat(\"My name is Dan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb98c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: content='<reasoning>We need to respond to \"I am a virgo\". The user gave astrological sign. Could respond with something about Virgo traits, ask about interest, or continue conversation. Keep friendly. Possibly ask if they want horoscope or personality traits. Should be concise.</reasoning>Nice to meet you, Dan! üåü As a Virgo, you‚Äôre probably detail‚Äëoriented, practical, and love a good sense of order. Does that sound about right? Let me know if you‚Äôd like to chat about anything‚Äîmaybe a daily horoscope, some Virgo‚Äëfocused tips, or something completely different!' additional_kwargs={'usage': {'prompt_tokens': 988, 'completion_tokens': 126, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 1114}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} response_metadata={'usage': {'prompt_tokens': 988, 'completion_tokens': 126, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 1114}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} id='run--e6968ff2-4e2e-4ce9-8f16-b3fe1b2d8ec6-0' usage_metadata={'input_tokens': 988, 'output_tokens': 126, 'total_tokens': 1114, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "chat(\"I am a virgo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa95401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: content='<reasoning>The user says \"Say my name\". They previously said their name is Dan. So we should respond by saying \"Your name is Dan.\" Possibly ask if they\\'d like anything else. Keep friendly.</reasoning>Your name is Dan. üòä Let me know if there‚Äôs anything else you‚Äôd like to talk about!' additional_kwargs={'usage': {'prompt_tokens': 1385, 'completion_tokens': 69, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 1454}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} response_metadata={'usage': {'prompt_tokens': 1385, 'completion_tokens': 69, 'cache_read_input_tokens': 0, 'cache_write_input_tokens': 0, 'total_tokens': 1454}, 'stop_reason': None, 'thinking': {}, 'model_id': 'openai.gpt-oss-120b-1:0', 'model_name': 'openai.gpt-oss-120b-1:0'} id='run--fbb9c612-543c-4653-83fd-4b6ffd757d56-0' usage_metadata={'input_tokens': 1385, 'output_tokens': 69, 'total_tokens': 1454, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "chat(\"Say my name\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
